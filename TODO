[i] scrape wikipedia using HTML, not wiki markup
  - too many random, quirky templates
  [x] separate branch
    html-based-scraping
  [i] convert one simple integration test, temporarily disable others
    [i] convert integration test
      [ ] find api call
      [ ] choose test to convert
      [ ] comment out others
      [ ] update relevant mocking
        [ ] url
        [ ] format
      [ ] update unit tests
        [ ] use beautiful soup to grab section
        [ ] html-sanitizer
        [ ] beautiful soup again to get text of paragraphs (and others)
          p blockquote ul ol dl
        [ ] remove square-bracketed citations as well as parenthesized expressions
  [ ] convert other integrations tests
    - TBD
  [ ] merge branch in
[ ] use studio voices
[ ] podcast on spotify
    [ ] spotify settings
    [ ] spreaker settings
[ ] consider sections when ordering articles
[ ] de-dupe articles
    [ ] by full reference (title and section)
[ ] iTunes
[ ] upgrade/remove libraries (Pydantic?)
[ ] upload in preview mode
    [ ] config flag (in terraform)
    [ ] flag controls uploading to spreaker in preview mode
[ ] Audible
[ ] re-assess voices
[ ] podcast on youtube
    [ ] channel
    [ ] spreaker settings
[ ] better error handling
[ ] exclude "list of" articles





import wikitextparser as wtp
import textwrap
source = textwrap.dedent(
    """
    It has a mean diameter of approximately {{convert|220|km|sp=us}} and
    contains about one percent of the mass of the [[asteroid belt]].
    """)
parsed = wtp.parse(source)
best = parsed.sections[0]
wtp.parse(best.contents).plain_text(
    replace_templates=lambda x: str(x.arguments[0].__dir__()))
